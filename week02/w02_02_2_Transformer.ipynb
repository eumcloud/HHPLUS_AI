{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# Transformer 실습\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "dc6c384c-1013-4faf-e107-777856438ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, sacremoses, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 sacremoses-0.1.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# # 데이터 로드 (5%만 사용)\n",
        "# train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
        "# test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")\n",
        "# 데이터 로드\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "# BERT tokenizer 불러오기\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "# Collate 함수 정의\n",
        "def collate_fn(batch):\n",
        "    max_len = 400\n",
        "    texts, labels = [], []\n",
        "    for row in batch:\n",
        "        texts.append(row['text'])\n",
        "        labels.append(row['label'])\n",
        "    texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return texts, labels\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFDZc-1ZyQtp",
        "outputId": "017ec0b9-9d3a-406c-f2dc-3a52111968b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "# Positional Encoding\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # ✅ Multi-Head 설정\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads  # ✅ 각 head의 차원\n",
        "\n",
        "        # 기존 Self-Attention과 동일한 q, k, v projection\n",
        "        self.wq = nn.Linear(input_dim, d_model)\n",
        "        self.wk = nn.Linear(input_dim, d_model)\n",
        "        self.wv = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model)  # ✅ head concat 후 projection\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        B, S, _ = x.size()  # B: batch size, S: seq len\n",
        "\n",
        "        # (B, S, D) → (B, H, S, D')로 변환\n",
        "        q = self.wq(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, S, D')\n",
        "        k = self.wk(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.wv(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Self-Attention과 동일한 방식으로 attention score 계산 (각 head별로 병렬)\n",
        "        score = torch.matmul(q, k.transpose(-1, -2)) / sqrt(self.d_head)  # (B, H, S, S)\n",
        "\n",
        "        # Mask shape도 (B, 1, S, S)로 맞춰줘야 head 차원과 브로드캐스팅 가능\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, :, :]  # (B, 1, S, S)\n",
        "            score = score.masked_fill(mask, -1e9)\n",
        "\n",
        "        # Softmax + Dropout\n",
        "        attn = self.dropout(self.softmax(score))\n",
        "\n",
        "        # attention 결과 계산: (B, H, S, D')\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # 여러 head를 다시 하나로 합치기: (B, H, S, D') → (B, S, D)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, S, self.n_heads * self.d_head)\n",
        "\n",
        "        # 마지막 projection\n",
        "        return self.wo(out)\n",
        "\n",
        "\n",
        "# Transformer Layer\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, dff, n_heads):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1 = self.norm1(x + self.dropout1(self.mha(x, mask)))\n",
        "        x2 = self.norm2(x1 + self.dropout2(self.ffn(x1)))\n",
        "        return x2\n",
        "\n",
        "# 전체 모델\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_layers, dff, n_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.parameter.Parameter(positional_encoding(400, d_model), requires_grad=False)\n",
        "        self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff, n_heads) for _ in range(n_layers)])\n",
        "        self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == tokenizer.pad_token_id)\n",
        "        mask = mask[:, None, :].expand(-1, x.size(1), -1)  # (B, S, S)\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) * sqrt(self.embedding.embedding_dim)\n",
        "        x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return self.classification(x[:, 0])\n"
      ],
      "metadata": {
        "id": "o34BeMAUyQ0V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "model = TextClassifier(len(tokenizer), d_model=128, n_layers=5, dff=256, n_heads=4).to('cuda')\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Accuracy 측정 함수\n",
        "def accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to('cuda'), y.to('cuda')\n",
        "            preds = (model(x) > 0).long().squeeze()\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# 학습 loop\n",
        "n_epochs = 5\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to('cuda'), y.float().to('cuda')\n",
        "        model.zero_grad()\n",
        "        preds = model(x).squeeze()\n",
        "        loss = loss_fn(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {total_loss:.4f} | Train Acc: {accuracy(model, train_loader):.3f} | Test Acc: {accuracy(model, test_loader):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FUFnobhyx4a",
        "outputId": "cd99675a-d3e0-41ce-c4d3-068bca0698b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train Loss: 210.5462 | Train Acc: 0.811 | Test Acc: 0.789\n",
            "Epoch 1 | Train Loss: 154.6481 | Train Acc: 0.854 | Test Acc: 0.819\n",
            "Epoch 2 | Train Loss: 137.3433 | Train Acc: 0.885 | Test Acc: 0.840\n",
            "Epoch 3 | Train Loss: 125.2467 | Train Acc: 0.850 | Test Acc: 0.814\n",
            "Epoch 4 | Train Loss: 118.3788 | Train Acc: 0.902 | Test Acc: 0.839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r8T5LOuMy5-T"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}