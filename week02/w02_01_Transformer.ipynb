{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# Transformer ì‹¤ìŠµ\n",
        "\n",
        "## ëª©í‘œ\n",
        "\n",
        "---\n",
        "\n",
        "- [ ]  Last word prediction dataset ì¤€ë¹„\n",
        "    - ê¸°ì¡´ì˜ IMDB datasetì„ ê·¸ëŒ€ë¡œ í™œìš©í•˜ê³ , `collate_fn`ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •\n",
        "        \n",
        "        ```python\n",
        "        from torch.nn.utils.rnn import pad_sequence\n",
        "        \n",
        "        def collate_fn(batch):\n",
        "          max_len = 400\n",
        "          texts, labels = [], []\n",
        "          for row in batch:\n",
        "            labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3])\n",
        "            texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
        "        \n",
        "          texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "          labels = torch.LongTensor(labels)\n",
        "        \n",
        "          return texts, labels\n",
        "        ```\n",
        "        \n",
        "- [ ]  Loss function ë° classifier output ë³€ê²½\n",
        "    - ë§ˆì§€ë§‰ token idë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— binary classificationì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ classification ë¬¸ì œë¡œ ë°”ë€œ. MNIST ê³¼ì œì—ì„œ í–ˆë˜ ê²ƒ ì²˜ëŸ¼ lossì™€ `TextClassifier`ì˜ ì¶œë ¥ ì°¨ì›ì„ ì˜ ì¡°ì •í•˜ì—¬ taskë¥¼ í’€ ìˆ˜ ìˆë„ë¡ ìˆ˜ì •\n",
        "- [ ]  í•™ìŠµ ê²°ê³¼ report\n",
        "    - ê¸°ì¡´ Transformer ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ë¡œ last word predictionì„ í•™ìŠµí•˜ê³  í•™ìŠµ ê²½ê³¼ë¥¼ report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IoFsJYn_Nnnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "9c808ab5-eccf-419f-e20a-c8f7227e7d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, sacremoses, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 sacremoses-0.1.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfb7d32-a326-4df1-a0a0-5c957aab362b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_len = 400\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for row in batch:\n",
        "        input_ids = tokenizer(row['text'], truncation=True, max_length=max_len).input_ids\n",
        "        if len(input_ids) < 4:\n",
        "            continue  # ìµœì†Œ ê¸¸ì´ í™•ë³´\n",
        "        labels.append(input_ids[-3])  # ë§ˆì§€ë§‰ ë‹¨ì–´ (ì˜ˆì¸¡ ëŒ€ìƒ)\n",
        "        texts.append(torch.LongTensor(input_ids[:-3]))  # ì…ë ¥ì—ì„œ ì œì™¸\n",
        "\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "CxI7Fjwa6piN",
        "outputId": "b31dbc32-16dd-4e4e-e54e-b076defb3523"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "output = tokenizer.encode(\"Hello, y'all! How are you ğŸ˜ ?\")\n",
        "output.tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRAGUiMXgXqE",
        "outputId": "823ba7bc-cc64-4f3b-b4c1-c8fb49654351"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'hello',\n",
              " ',',\n",
              " 'y',\n",
              " \"'\",\n",
              " 'all',\n",
              " '!',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " '[UNK]',\n",
              " '?',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from tokenizers import Tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "x = torch.LongTensor(tokenizer.encode(\"Hello, y'all! How are you ğŸ˜ ?\").ids)[None]\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Dul6q_gbVR",
        "outputId": "d06115d1-1b79-46da-d071-94f4f8942800"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 7592, 1010, 1061, 1005, 2035,  999, 2129, 2024, 2017,  100, 1029,\n",
              "          102]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(tokenizer.get_vocab_size(), 256)\n",
        "embedding(x[None]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-bMo8rpg79z",
        "outputId": "92e72a83-13a4-4bf2-d55e-8af98062a655"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 13, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wKK4M6Xg8Ae",
        "outputId": "25aee8b1-b896-4e9d-b7ec-d332723464ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 101, 7592, 1010, 1061, 1005, 2035,  999, 2129, 2024, 2017,  100, 1029,\n",
            "         102])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    score = self.softmax(score)\n",
        "    result = torch.matmul(score, v)\n",
        "    result = self.dense(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model)\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask)\n",
        "    x = self.ffn(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "277ebea6-7db2-4aef-e64b-6af54a74c66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "\n",
        "    # self.classification = nn.Linear(d_model, 1)\n",
        "    self.classification = nn.Linear(d_model, vocab_size) # vocab_sizeë¡œ ë³€ê²½\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# model = TextClassifier(len(tokenizer), 32, 2, 32)\n",
        "model = TextClassifier(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=128,\n",
        "    n_layers=4,\n",
        "    dff=512\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## í•™ìŠµ\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "# loss_fn = nn.BCEWithLogitsLoss()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "    cnt = 0\n",
        "    acc = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)\n",
        "        preds = torch.argmax(preds, dim=-1)  # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ê¸°ì¤€\n",
        "\n",
        "        cnt += labels.shape[0]\n",
        "        acc += (labels == preds).sum().item()\n",
        "\n",
        "    return acc / cnt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "al_b56TYRILq",
        "outputId": "6160bb29-c78d-4f2b-84aa-50db23ea77cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 2968.5435452461243\n",
            "=========> Train acc: 0.035 | Test acc: 0.033\n",
            "Epoch   1 | Train Loss: 2682.4187622070312\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   2 | Train Loss: 2660.7557015419006\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   3 | Train Loss: 2654.480152606964\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n",
            "Epoch   4 | Train Loss: 2650.774440765381\n",
            "=========> Train acc: 0.039 | Test acc: 0.039\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c9cabb1820e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# preds = model(inputs)[..., 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (B, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# labels: (B,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-39656d1414f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9c7ef31e41de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f5568a6a03f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda') #.float()\n",
        "\n",
        "    # preds = model(inputs)[..., 0]\n",
        "    preds = model(inputs)  # shape: (B, vocab_size)\n",
        "    loss = loss_fn(preds, labels)  # labels: (B,)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í•™ìŠµíš¨ìœ¨ ë¬¸ì œ\n",
        "\n",
        "* ì™œ í•„í„°ë§ì´ í•„ìš”í• ê¹Œ?\n",
        "- TransformerëŠ” ê°•ë ¥í•˜ê¸´ í•˜ì§€ë§Œ í•™ìŠµ íš¨ìœ¨ì´ ë‹¤ìŒì˜ ìš”ì†Œì— ë§¤ìš° ë¯¼ê°:\n",
        "\n",
        "1. ë¼ë²¨ í´ë˜ìŠ¤ ìˆ˜ê°€ ë„ˆë¬´ ë§ì„ ë•Œ â†’ í•™ìŠµ signalì´ í¬ì„ë¨\n",
        "\n",
        "2. ë¼ë²¨ ë¶„í¬ê°€ í•œìª½ìœ¼ë¡œ ì ë ¤ìˆì„ ë•Œ â†’ ëª¨ë¸ì´ íŠ¹ì • í† í°ë§Œ ì˜ˆì¸¡í•˜ë ¤ í•¨\n",
        "\n",
        "3. ë¶ˆí•„ìš”í•˜ê±°ë‚˜ í•™ìŠµì— ë„ì›€ ì•ˆ ë˜ëŠ” ì˜ˆì‹œê°€ ë§ì„ ë•Œ â†’ í•™ìŠµ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„"
      ],
      "metadata": {
        "id": "NFfsNr-LWWnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì •í™•íˆ ì§šìœ¼ì…¨ì–´ìš” ğŸ‘  \n",
        "ì§€ê¸ˆ í•˜ëŠ” **â€œë§ˆì§€ë§‰ ë‹¨ì–´ ì˜ˆì¸¡â€** taskëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì¤‘ì—ì„œë„ **ê°€ì¥ ë‚œì´ë„ê°€ ë†’ì€ ì¢…ë¥˜**ì˜ˆìš”. ë‹¨ì–´ ìˆ˜(í´ë˜ìŠ¤ ìˆ˜)ê°€ ìˆ˜ë§Œ ê°œê³ , ë¬¸ì¥ ë ë‹¨ì–´ëŠ” ì˜ˆì¸¡ì´ íŠ¹íˆ ì–´ë µìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ë˜ì„œ **\"ë°ì´í„° í•„í„°ë§\"**ì„ í†µí•´ í•™ìŠµ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê±´ ë§¤ìš° ì¢‹ì€ ì ‘ê·¼ì´ê³ , ì—¬ëŸ¬ ë°©ë²•ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì•„ë˜ì—ì„œ **ì „ëµë³„ ê³ ë¯¼ í¬ì¸íŠ¸ + í•„í„°ë§ ë°©ì‹ + ì¥ë‹¨ì **ì„ í•¨ê»˜ ì •ë¦¬í• ê²Œìš”.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§° **ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í•„í„°ë§ ì „ëµ**\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Rare token ì œê±°**  \n",
        "\n",
        "**ì•„ì´ë””ì–´:**  \n",
        "ì „ì²´ datasetì„ ìŠ¤ìº”í•´ì„œ ë§ˆì§€ë§‰ ë‹¨ì–´(ì˜ˆì¸¡ ëŒ€ìƒ) ì¤‘ ë„ˆë¬´ ë“œë¬¼ê²Œ ë‚˜ì˜¤ëŠ” í† í°ì€ ì œê±°í•˜ì.\n",
        "\n",
        "**ì ìš© ì´ìœ :**  \n",
        "ë“œë¬¼ê²Œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ëª¨ë¸ì´ í•™ìŠµí•  ê¸°íšŒë„ ì ê³ , í™•ë¥ ì ìœ¼ë¡œ ë§ì¶œ ê°€ëŠ¥ì„±ë„ ê±°ì˜ ì—†ìŒ. ì´ê±¸ ê³„ì† í•™ìŠµí•˜ê²Œ ë‘ë©´ **lossëŠ” ì»¤ì§€ê³  gradientëŠ” noiseë§Œ ìƒê¹€**.\n",
        "\n",
        "**ì¥ì :**\n",
        "- ì„±ëŠ¥ ì•ˆì •í™”\n",
        "- í•™ìŠµ íš¨ìœ¨ ëŒ€í­ í–¥ìƒ\n",
        "- loss í­ ê°ì†Œ\n",
        "\n",
        "**ë‹¨ì :**\n",
        "- ë“œë¬¸ but ì¤‘ìš”í•œ í† í° ì˜ˆì¸¡ ëŠ¥ë ¥ì€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ\n",
        "- ë¼ë²¨ bias ìƒê¸¸ ìˆ˜ ìˆìŒ (ìì£¼ ë‚˜ì˜¤ëŠ” ê²ƒë§Œ ë§ì¶”ëŠ” ìŠµê´€ ìƒê¹€)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **ì§§ì€ ë¬¸ì¥ ì œê±°**\n",
        "\n",
        "**ì•„ì´ë””ì–´:**  \n",
        "ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ (ì˜ˆ: 5ë‹¨ì–´ ì´í•˜)ì€ contextê°€ ë¶€ì¡±í•´ì„œ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì¶”ì¸¡í•  ì •ë³´ê°€ ì—†ìŒ.\n",
        "\n",
        "**ì ìš© ì˜ˆì‹œ:**\n",
        "```python\n",
        "if len(input_ids) < 10: continue\n",
        "```\n",
        "\n",
        "**ì¥ì :**\n",
        "- ë¬¸ë§¥ ë¶€ì¡±í•œ input ì œê±° â†’ ëª¨ë¸ì´ ë¬´ì˜ë¯¸í•œ í•™ìŠµ ì•ˆ í•¨\n",
        "- positional encoding, attention íš¨ê³¼ ì¦ê°€\n",
        "\n",
        "**ë‹¨ì :**\n",
        "- ë°ì´í„° ìˆ˜ê°€ ì¤„ì–´ë“¦\n",
        "- ë¬¸ì¥ ê¸¸ì´ ë‹¤ì–‘ì„± ì†ì‹¤\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Stopword / êµ¬ë‘ì  ì œê±°**\n",
        "\n",
        "**ì•„ì´ë””ì–´:**  \n",
        "ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ '.', ',', 'the', 'a', 'I' ê°™ì€ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì˜ˆì¸¡ ì˜ë¯¸ê°€ ì—†ëŠ” í† í°ì´ë©´ ì œê±°\n",
        "\n",
        "**ì ìš© ì˜ˆì‹œ:**\n",
        "```python\n",
        "if tokenizer.decode([label]).lower() in stop_words:\n",
        "    continue\n",
        "```\n",
        "\n",
        "**ì¥ì :**\n",
        "- ì‹¤ì œë¡œ ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ë§Œ ë‚¨ìŒ\n",
        "- í•™ìŠµëœ ëª¨ë¸ì´ ë¬¸ì¥ ìƒì„±/ì˜ˆì¸¡ì—ì„œ í›¨ì”¬ ìì—°ìŠ¤ëŸ¬ì›€\n",
        "\n",
        "**ë‹¨ì :**\n",
        "- stopword ì •ì˜ê°€ ì–´ë µê³  language-specific\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## âœ… **\"ë¹ˆë„ìˆ˜ ê¸°ë°˜ rare token ì œê±°\"**\n",
        "\n",
        "- ì½”ë“œ êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ ì ìš© ê°€ëŠ¥\n",
        "- vocab ìˆ˜ë¥¼ ì¤„ì—¬ì„œ **softmax ì—°ì‚° ë¶€ë‹´ ì™„í™”**\n",
        "- ì •í™•ë„ì™€ loss ê°œì„ ì„ **ë‹¨ê¸°ê°„ì— ì²´ê°í•  ìˆ˜ ìˆìŒ**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” ê²°ë¡  & ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "| ëª©ì  | ë°©ë²• | ì‹¤í–‰ ë‚œì´ë„ | ì¶”ì²œ ìˆœìœ„ |\n",
        "|------|------|--------------|------------|\n",
        "| ë¼ë²¨ ë‹¤ì–‘ì„± ì¤„ì´ê¸° | ë¹ˆë„ìˆ˜ ê¸°ë°˜ rare token ì œê±° | â­ï¸ ì‰½ë‹¤ | âœ… 1ìˆœìœ„ |\n",
        "| ì˜ë¯¸ ì—†ëŠ” ë¬¸ì¥ ì œê±° | ì§§ì€ ë¬¸ì¥ ì œê±° | â­ï¸ ì‰½ë‹¤ | âœ… 1~2ìˆœìœ„ |\n",
        "| noise ì œê±° | stopword í•„í„°ë§ | âš ï¸ ì¤‘ê°„ | â³ ì‹¤í—˜ í•„ìš” |\n",
        "| ì–¸ì–´ëª¨ë¸ pretrainì²˜ëŸ¼ | ì¤‘ê°„ ìœ„ì¹˜ ë‹¨ì–´ ì˜ˆì¸¡ | âš ï¸ ì–´ë µë‹¤ | â³ êµ¬ì¡° ë³€ê²½ í•„ìš” |\n",
        "\n",
        "---\n",
        ""
      ],
      "metadata": {
        "id": "eK1-FOQIWjRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í•™ìŠµ ë°ì´í„° í•„í„°ë§ : **Rare token ì œê±°**  \n",
        "collate_fnì—ì„œ ë„ˆë¬´ ì§§ê±°ë‚˜ rareí•œ labelì€ ì œì™¸:"
      ],
      "metadata": {
        "id": "BCRLygVpXsbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# ì‚¬ì „ì ìœ¼ë¡œ label ë¹ˆë„ìˆ˜ êµ¬í•´ë†“ê³ \n",
        "label_counts = Counter()\n",
        "\n",
        "for row in train_ds:\n",
        "    input_ids = tokenizer(row['text'], truncation=True, max_length=400).input_ids\n",
        "    if len(input_ids) >= 4:\n",
        "        label_counts[input_ids[-3]] += 1\n",
        "\n",
        "common_labels = set([tok for tok, cnt in label_counts.items() if cnt > 10])  # ìµœì†Œ 10ë²ˆ ì´ìƒ ë“±ì¥í•œ labelë§Œ\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_len = 400\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for row in batch:\n",
        "        input_ids = tokenizer(row['text'], truncation=True, max_length=max_len).input_ids\n",
        "        if len(input_ids) < 4:\n",
        "            continue\n",
        "        label = input_ids[-3]\n",
        "        if label not in common_labels:\n",
        "            continue  # rare tokenì´ë©´ ì œì™¸\n",
        "        labels.append(label)\n",
        "        texts.append(torch.LongTensor(input_ids[:-3]))\n",
        "\n",
        "    if len(texts) == 0:\n",
        "        return None  # skip batch\n",
        "\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "fyYVTnNKXuQb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassifier(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=64,\n",
        "    n_layers=2,\n",
        "    dff=256\n",
        ")\n",
        "\n",
        "model = model.to('cuda')\n",
        "\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    if data is None or data[0].numel() == 0:\n",
        "        continue  # â† ë¹ˆ ë°°ì¹˜ skip\n",
        "\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda') #.float()\n",
        "\n",
        "    # preds = model(inputs)[..., 0]\n",
        "    preds = model(inputs)  # shape: (B, vocab_size)\n",
        "    loss = loss_fn(preds, labels)  # labels: (B,)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "a0lA5yaHX4Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# of training examples: {len(train_loader.dataset)}\")\n",
        "print(\"# Train examples:\", sum(1 for _ in train_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtBU3Z6SYOcW",
        "outputId": "59f8e186-dc9b-4bc3-ef46-e6b8c32fd72b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of training examples: 25000\n",
            "# Train examples: 391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rvF6XDzeIUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}